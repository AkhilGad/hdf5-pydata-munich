{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab-completion for groups and attributes\n",
    "?h5py.enable_ipython_completer\n",
    "h5py.enable_ipython_completer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jack/Repos/hdf5-pydata-munich/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The h5py library is a thin, pythonic wrapper around the HDF5 C API.\n",
    "\n",
    "It tries to expose most of the functionality that the HDF5 library provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_dataset(name='my_dataset', data=[1.0, 2.7, 3.7, 4.5])\n",
    "#     f.create_dataset(name='my_other_dataset', data=[1, 2, 3, 4])\n",
    "#     f.create_dataset(name='my_other_dataset', data=[1, 2, 3, 4], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"my_dataset\": shape (4,), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    # the array is just a proxy object\n",
    "    print(f['my_dataset'])\n",
    "    # the actual data can be accessed with these 2 syntaxes\n",
    "#     print(f['my_dataset'][:])\n",
    "#     print(f['my_dataset'][...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preallocation on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    ds = f.create_dataset(name='my_dataset', shape=(8, 1))\n",
    "    ds[0] = 5.2\n",
    "    ds[1] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the correct HDF5 datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 254 255   0 255 254]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0, 1, 254, 255, 256, -1, -2], dtype='uint8')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 254 255 255   0   0]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_dataset(name='my_dataset', shape=(7,), dtype=h5py.h5t.STD_U8BE)\n",
    "    f['my_dataset'][0:8] = [0, 1, 254, 255, 123456, -1, -2]\n",
    "    print(f[\"my_dataset\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_group(name='group1')\n",
    "    group2 = f.create_group(name='group2')\n",
    "    group2.create_group(name='group3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/group2\" (1 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    group3 = f['group2/group3']\n",
    "    print(group3.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    ds = f.create_dataset(name='my_dataset', data=[1, 2, 3, 4])\n",
    "    ds.attrs['Unit'] = 'm/s'\n",
    "    gr = f.create_group(name='my_group')\n",
    "    gr.attrs['Created'] = '18/12/2017'\n",
    "    gr.attrs.create(name='Versions', data=np.array([123, 456])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse a HDF5 file with h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dataset\n",
      "my_group\n"
     ]
    }
   ],
   "source": [
    "def print_name(name):\n",
    "    print(name)\n",
    "\n",
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    f.visit(print_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 Command Line Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://support.hdfgroup.org/products/hdf5_tools/#h5dist) you can find the command line tools developed by the HDF Group. You don't need h5py or PyTables to use them.\n",
    "\n",
    "If you are on Ubuntu, you can install them with `sudo apt install hdf5-tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\r\n",
      "/my_dataset              Dataset {4}\r\n",
      "/my_group                Group\r\n"
     ]
    }
   ],
   "source": [
    "# -r stands for 'recursive'\n",
    "!h5ls -r 'data/my_h5py_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"data/my_h5py_file.h5\" {\r\n",
      "GROUP \"/\" {\r\n",
      "   DATASET \"my_dataset\" {\r\n",
      "      DATATYPE  H5T_STD_I64LE\r\n",
      "      DATASPACE  SIMPLE { ( 4 ) / ( 4 ) }\r\n",
      "      DATA {\r\n",
      "      (0): 1, 2, 3, 4\r\n",
      "      }\r\n",
      "      ATTRIBUTE \"Unit\" {\r\n",
      "         DATATYPE  H5T_STRING {\r\n",
      "            STRSIZE H5T_VARIABLE;\r\n",
      "            STRPAD H5T_STR_NULLTERM;\r\n",
      "            CSET H5T_CSET_UTF8;\r\n",
      "            CTYPE H5T_C_S1;\r\n",
      "         }\r\n",
      "         DATASPACE  SCALAR\r\n",
      "         DATA {\r\n",
      "         (0): \"m/s\"\r\n",
      "         }\r\n",
      "      }\r\n",
      "   }\r\n",
      "   GROUP \"my_group\" {\r\n",
      "      ATTRIBUTE \"Created\" {\r\n",
      "         DATATYPE  H5T_STRING {\r\n",
      "            STRSIZE H5T_VARIABLE;\r\n",
      "            STRPAD H5T_STR_NULLTERM;\r\n",
      "            CSET H5T_CSET_UTF8;\r\n",
      "            CTYPE H5T_C_S1;\r\n",
      "         }\r\n",
      "         DATASPACE  SCALAR\r\n",
      "         DATA {\r\n",
      "         (0): \"18/12/2017\"\r\n",
      "         }\r\n",
      "      }\r\n",
      "      ATTRIBUTE \"Versions\" {\r\n",
      "         DATATYPE  H5T_STD_I64LE\r\n",
      "         DATASPACE  SIMPLE { ( 2 ) / ( 2 ) }\r\n",
      "         DATA {\r\n",
      "         (0): 123, 456\r\n",
      "         }\r\n",
      "      }\r\n",
      "   }\r\n",
      "}\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!h5dump 'data/my_h5py_file.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables provides a higher abstraction over HDF5. This doesn't make it slower than h5py though.\n",
    "\n",
    "At the moment PyTables does **not** depend on h5py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', \n",
    "                   name='my_array',\n",
    "#                    name='my-array', # NaturalNameWarning\n",
    "#                    title='My PyTables Array',\n",
    "                   obj=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables has a feature called \"Natural Naming\": nodes (i.e. datasets and groups in the HDF5 file) can be accessed with the dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/my_array (Array(4,)) ''\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f:\n",
    "    print(f.root.my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_group(where='/', name='my_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where=f.root, name='my_array', obj=[1, 2, 3, 4], title='My PyTables Array')\n",
    "    f.set_node_attr(where='/my_array', attrname='SomeAttribute', attrvalue='SomeValue')\n",
    "    f.create_group(where='/', name='my_group')\n",
    "    f.set_node_attr(where='/my_group', attrname='SomeOtherAttribute', attrvalue=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 datasets have many abstractions in PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogenous dataset:\n",
    "\n",
    "- **Array**\n",
    "- **CArray**\n",
    "- **EArray**\n",
    "- **VLArray**\n",
    "\n",
    "Heterogenous dataset:\n",
    "\n",
    "- **Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1000000  # 1 million\n",
    "num_columns = 5\n",
    "gaussian = np.random.normal(loc=0, scale=1, size=num_rows).astype('float32')\n",
    "uniform = np.random.uniform(low=100, high=150, size=num_rows).astype('uint8')\n",
    "matrix = np.random.random((num_rows, num_columns)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array (again!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-array-class)\n",
    "\n",
    "- Fastest I/O speed\n",
    "- Homogeneous (i.e. data has same `dtype`)\n",
    "- Must fit in memory\n",
    "- Not compressible\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 140 ms, total: 152 ms\n",
      "Wall time: 150 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', name='gaussian', obj=gaussian)\n",
    "    f.create_array(where='/', name='uniform', obj=uniform)\n",
    "    f.create_array(where='/', name='matrix', obj=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#carrayclassdescr)\n",
    "\n",
    "- Chunked storage\n",
    "- Data must be homogeneous\n",
    "- Good speed when reading/writing\n",
    "- Compressible\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_carray(where='/', name='my_carray', obj=[1, 2, 3, 4])\n",
    "    # you can create a CArray and fill it later, but you need to specify atom and shape\n",
    "    carray = f.create_carray(where='/', name='my_other_carray', atom=tb.Float32Atom(), shape=(4, 2))\n",
    "    # later...\n",
    "    carray[:, 1] = [5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tb.Filters(complevel=5, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips on how to use compression (from the PyTables docs)\n",
    "\n",
    "- A mid-level (5) compression is sufficient. No need to go all the way up (9)\n",
    "- Use zlib if you must guarantee complete portability\n",
    "- Use blosc all other times (it is optimized for HDF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 872 ms, sys: 80 ms, total: 952 ms\n",
      "Wall time: 952 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_carray(where='/', name='gaussian', obj=gaussian, filters=filters)\n",
    "    f.create_carray(where='/', name='uniform', obj=uniform, filters=filters)\n",
    "    f.create_carray(where='/', name='matrix', obj=matrix, filters=filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#earrayclassdescr)\n",
    "\n",
    "- Enlargeable on **one** dimension (append)\n",
    "- Pretty fast at extending, very good at reading\n",
    "- Data must be homogeneous\n",
    "- Compressible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 840 ms, sys: 100 ms, total: 940 ms\n",
      "Wall time: 942 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# One (and only one) of the shape dimensions *must* be 0.\n",
    "# The dimension being 0 means that the resulting EArray object can be extended along it.\n",
    "# Multiple enlargeable dimensions are not supported right now.\n",
    "shape = (num_rows, 0)\n",
    "\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    earray = f.create_earray(where='/',\n",
    "                             name='my_earray',\n",
    "                             atom=tb.Float32Atom(),\n",
    "                             shape=shape,\n",
    "                             filters=filters)\n",
    "    earray.append(sequence=matrix[:, 0:1])\n",
    "    earray.append(sequence=matrix[:, 1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-vlarray-class)\n",
    "\n",
    "- Supports collections of homogeneous data with a variable number of entries\n",
    "- Compressible\n",
    "- Enlargeable (append)\n",
    "- I/O is not very fast\n",
    "- Like Table datasets, variable length arrays can have only one dimension, and the elements (atoms) of their rows can be fully multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    vlarray = f.create_vlarray(where=f.root, name='my_vlarray', atom=tb.Float32Atom())\n",
    "    vlarray.append(gaussian[0:10])\n",
    "    vlarray.append(uniform[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/structured_storage.html?highlight=table#tableclassdescr)\n",
    "\n",
    "- Data can be heterogeneous (i.e. different shapes and different dtypes)\n",
    "- The structure of a table is declared by its description\n",
    "- It supports *in-kernel* searches with `Table.where`\n",
    "- It supports multi-column searches\n",
    "- Non-nested columns can be *indexed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to emulate in Python records mapped to HDF5 C structs PyTables implements a special class so as to easily define all its fields and other properties. It's called `IsDescription`.\n",
    "\n",
    "A *description* defines the table structure (basically, the *schema* of your table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle(tb.IsDescription):\n",
    "    identity = tb.StringCol(itemsize=22, dflt=' ', pos=0)  # character String\n",
    "    idnumber = tb.Int16Col(dflt=1, pos=1)  # short integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identity': StringCol(itemsize=22, shape=(), dflt=b' ', pos=0), 'idnumber': Int16Col(shape=(), dflt=1, pos=1)}\n"
     ]
    }
   ],
   "source": [
    "print(Particle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    table = f.create_table(where='/', name='my_table', description=Particle)\n",
    "    \n",
    "    num_rows = 100\n",
    "    row = table.row\n",
    "    for i in range(num_rows):\n",
    "        row['identity'] = 'I am {}'.format(i)\n",
    "        row['idnumber'] = i\n",
    "        row.append()\n",
    "    # Flush the table buffers to release memory and make sure are written to disk\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description can be nested inside another description (this will look weird...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle(tb.IsDescription):\n",
    "    identity = tb.StringCol(itemsize=22, dflt=' ', pos=0)\n",
    "    idnumber = tb.Int16Col(dflt=1, pos=1)\n",
    "\n",
    "    class Properties(tb.IsDescription):\n",
    "        # 2-D float array (single-precision)\n",
    "        pressure = tb.Float32Col(shape=(2, 3))\n",
    "        # 3-D float array (double-precision)\n",
    "        energy = tb.Float64Col(shape=(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file('data/my_pytables_file.h5', 'w') as f:\n",
    "    table = f.create_table(where='/', name='my_table', description=Particle)\n",
    "    \n",
    "    num_rows = 100\n",
    "    row = table.row\n",
    "    for i in range(num_rows):\n",
    "        row['identity'] = 'I am {}'.format(i)\n",
    "        row['idnumber'] = i\n",
    "        row['Properties/pressure'] = np.random.random(size=(2, 3))\n",
    "        row['Properties/energy'] = np.random.random(size=(2, 3, 4))\n",
    "        row.append()\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse a HDF5 file with PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/my_table\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f:\n",
    "    for node in f.walk_nodes('/', classname='Table'):\n",
    "        print('{}'.format(node._v_pathname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Yellow Taxi Dataset (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big data, big deal](https://marktortorici.files.wordpress.com/2013/10/ron-burgundy-big-deal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data - without having to load into memory an entire 2GB CSV file - with the unix `less` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`less yellow_tripdata_2015-01.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these CSV files there are some changes from year to year. A few things I noticed:\n",
    "\n",
    "- in the CSV files from 2014 there is a field called `pickup_datetime`. From January 2015 onwards this field has been renamed as `tpep_pickup_datetime`;\n",
    "- starting from July 2016 the columns `pickup_longitude` and `pickup_latitude` have been replaced with `PULocationID`, and the columns `dropoff_longitude` and `dropoff_latitude` with `DOLocationID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read/Store the CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These CSV files are huge! Don't forget the pandas [rule of thumb](http://wesmckinney.com/blog/apache-arrow-pandas-internals/):\n",
    "\n",
    "> Have 5 to 10 times as much RAM as the size of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop (Thinkpad X220 i5 10GB RAM) it took roughly:\n",
    "\n",
    "- **20 minutes** to read/store a **single CSV**.\n",
    "\n",
    "- **4 hours** to read/store an entire **year**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/20fb1g.jpg\" title=\"made at imgflip.com\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_2015_01           Dataset {12748986/Inf}\n",
      "yellow_2015_02           Dataset {12450521/Inf}\n",
      "yellow_2015_03           Dataset {13351609/Inf}\n",
      "yellow_2015_04           Dataset {13071789/Inf}\n",
      "yellow_2015_05           Dataset {13158262/Inf}\n",
      "yellow_2015_06           Dataset {12324935/Inf}\n",
      "yellow_2015_07           Dataset {11562783/Inf}\n",
      "yellow_2015_08           Dataset {11130304/Inf}\n",
      "yellow_2015_09           Dataset {11225063/Inf}\n",
      "yellow_2015_10           Dataset {12315488/Inf}\n",
      "yellow_2015_11           Dataset {11312676/Inf}\n",
      "yellow_2015_12           Dataset {11460573/Inf}\n"
     ]
    }
   ],
   "source": [
    "!h5ls 'data/NYC-yellow-taxis.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables is shipped with some useful command line utilities. These CLI utils are in `tables/utils`.\n",
    "\n",
    "You can use them if you are working in a python environment where you installed PyTables (these CLI utils cause your python interpreter to execute a python script in `tables/scripts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n",
      "/ (RootGroup)\r\n",
      "+--yellow_2015_03 (Table)\r\n",
      "|     mem=680.9MB, disk=312.9MB [ 9.3%]\r\n",
      "+--yellow_2015_05 (Table)\r\n",
      "|     mem=671.1MB, disk=307.0MB [ 9.1%]\r\n",
      "+--yellow_2015_04 (Table)\r\n",
      "|     mem=666.7MB, disk=305.5MB [ 9.1%]\r\n",
      "+--yellow_2015_01 (Table)\r\n",
      "|     mem=650.2MB, disk=294.6MB [ 8.7%]\r\n",
      "+--yellow_2015_02 (Table)\r\n",
      "|     mem=635.0MB, disk=290.5MB [ 8.6%]\r\n",
      "+--yellow_2015_06 (Table)\r\n",
      "|     mem=628.6MB, disk=287.7MB [ 8.5%]\r\n",
      "+--yellow_2015_10 (Table)\r\n",
      "|     mem=628.1MB, disk=280.0MB [ 8.3%]\r\n",
      "+--yellow_2015_07 (Table)\r\n",
      "|     mem=589.7MB, disk=262.6MB [ 7.8%]\r\n",
      "+--yellow_2015_12 (Table)\r\n",
      "|     mem=584.5MB, disk=260.7MB [ 7.7%]\r\n",
      "+--yellow_2015_11 (Table)\r\n",
      "|     mem=576.9MB, disk=257.5MB [ 7.6%]\r\n",
      "+--yellow_2015_09 (Table)\r\n",
      "|     mem=572.5MB, disk=255.6MB [ 7.6%]\r\n",
      "`--yellow_2015_08 (Table)\r\n",
      "      mem=567.6MB, disk=253.7MB [ 7.5%]\r\n",
      "\r\n",
      "------------------------------------------------------------\r\n",
      "Total branch leaves:    12\r\n",
      "Total branch size:      7.5GB in memory, 3.4GB on disk\r\n",
      "Mean compression ratio: 0.45\r\n",
      "HDF5 file size:         3.4GB\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pttree --use-si-units --sort-by 'size' 'data/NYC-yellow-taxis.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![He Man I have the data](https://www.storegrowers.com/wp-content/uploads/2016/02/analytics-meme-sword-guy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_where = '/yellow_2015_01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.read` reads the **entire table** (it must fit into memory) and query it with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 1min 22s, sys: 772 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.read() \n",
    "               if 1 < x['passenger_count'] < 4 and x['trip_distance'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.iterrows` returns an **iterator** that iterates over all rows (so no need to load the entire table into memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 6.56 s, sys: 484 ms, total: 7.04 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.iterrows()\n",
    "               if 1 < x['passenger_count'] < 4 and x['trip_distance'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"\"\"((1 < passenger_count) & (passenger_count < 4)) & (trip_distance > 0.5)\"\"\"\n",
    "# Of course you can make it more readable\n",
    "# cond0 = '((1 < passenger_count) & (passenger_count < 4))'\n",
    "# cond1 = '(trip_distance > 0.5)'\n",
    "# condition = '{} & {}'.format(cond0, cond1)\n",
    "# This won't work: you can't use Python's standard boolean operators in NumExpr expressions\n",
    "# condition = \"\"\"(1 < passenger_count < 4) & (trip_distance > 0.5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 16.2 s, sys: 388 ms, total: 16.6 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.read_where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.where` uses **NumExpr** to make a **in-kernel** query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 6.29 s, sys: 160 ms, total: 6.45 s\n",
      "Wall time: 6.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Indexes to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_i_yellow_2015_01        Group\n",
      "yellow_2015_01           Dataset {12748986/Inf}\n",
      "yellow_2015_02           Dataset {12450521/Inf}\n",
      "yellow_2015_03           Dataset {13351609/Inf}\n",
      "yellow_2015_04           Dataset {13071789/Inf}\n",
      "yellow_2015_05           Dataset {13158262/Inf}\n",
      "yellow_2015_06           Dataset {12324935/Inf}\n",
      "yellow_2015_07           Dataset {11562783/Inf}\n",
      "yellow_2015_08           Dataset {11130304/Inf}\n",
      "yellow_2015_09           Dataset {11225063/Inf}\n",
      "yellow_2015_10           Dataset {12315488/Inf}\n",
      "yellow_2015_11           Dataset {11312676/Inf}\n",
      "yellow_2015_12           Dataset {11460573/Inf}\n"
     ]
    }
   ],
   "source": [
    "!h5ls 'data/NYC-yellow-taxis-indexed.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 1min 26s, sys: 860 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.read() \n",
    "               if 1 < x['passenger_count'] < 4 and x['trip_distance'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 6.7 s, sys: 512 ms, total: 7.21 s\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.iterrows()\n",
    "               if 1 < x['passenger_count'] < 4 and x['trip_distance'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 17.4 s, sys: 468 ms, total: 17.9 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.read_where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 2218407\n",
      "CPU times: user 7.92 s, sys: 300 ms, total: 8.22 s\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotcha:** indexes might be slower if you have many results!\n",
    "\n",
    "[search with index is slower than without index?](https://stackoverflow.com/questions/20769818/search-with-index-is-slower-than-without-index-in-pytables-when-the-result-is-la)\n",
    "\n",
    "Let's try with a **more restrictive** condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"\"\"((passenger_count > 3)) & (trip_distance > 30)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 265\n",
      "CPU times: user 6.72 s, sys: 144 ms, total: 6.86 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 265\n",
      "CPU times: user 2.01 s, sys: 52 ms, total: 2.06 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    results = [x['total_amount'] for x in table.where(condition)]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jackdbd\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "dotenv_path = '.env'\n",
    "load_dotenv(dotenv_path)\n",
    "print(os.environ['PLOTLY_USERNAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_where = '/yellow_2015_01'\n",
    "condition = \"\"\"((passenger_count > 3)) & (trip_distance > 20.5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 4472\n",
      "CPU times: user 6.55 s, sys: 172 ms, total: 6.72 s\n",
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/NYC-yellow-taxis-indexed.h5', mode='r') as f:\n",
    "    table = f.get_node(where=table_where)\n",
    "    coordinates = [(x['pickup_latitude'], x['pickup_longitude']) for x in table.where(condition)]\n",
    "    print('Matches: {}'.format(len(coordinates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40.64424514770508, 40.64564895629883)\n",
      "(-73.7822036743164, -73.78545379638672)\n"
     ]
    }
   ],
   "source": [
    "latitudes, longitudes = zip(*coordinates)\n",
    "print(latitudes[:2])\n",
    "print(longitudes[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jackdbd/39.embed\" height=\"600px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import Scattermapbox, Scattergl, Marker, Data, Layout\n",
    "\n",
    "py.sign_in(os.environ['PLOTLY_USERNAME'], os.environ['PLOTLY_API_KEY'])\n",
    "mapbox_access_token = os.environ['MAPBOX_ACCESS_TOKEN']\n",
    "\n",
    "data = Data([\n",
    "    \n",
    "    Scattermapbox(\n",
    "        lat=latitudes,\n",
    "        lon=longitudes,\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=10,\n",
    "            color='rgb(247, 202, 24)',  # http://www.flatuicolorpicker.com/category/yellow\n",
    "            opacity=0.7,\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "#     Scattergl(\n",
    "#         x=longitudes,\n",
    "#         y = latitudes,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             line=dict(\n",
    "#                 width=1, \n",
    "#                 color='#404040',\n",
    "#             ),\n",
    "#         ),\n",
    "#     ),\n",
    "    \n",
    "])\n",
    "\n",
    "layout = Layout(\n",
    "    title='New York City yellow taxis pickup locations',\n",
    "    autosize=True,\n",
    "    height=600,\n",
    "    hovermode='closest',\n",
    "    showlegend=False,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        center=dict(\n",
    "            lat=40.75,\n",
    "            lon=-74.0\n",
    "        ),\n",
    "        zoom=10,\n",
    "        style='dark',\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to HDF5](https://www.youtube.com/watch?v=BAjsCldRMMc) by Quincey Koziol\n",
    "- [HDF5 is Eating the World](https://www.youtube.com/watch?v=nddj5OA8LJo) by Andrew Collette\n",
    "- [HDF5 take 2 - h5py & PyTables](https://www.youtube.com/watch?v=ofLFhQ9yxCw) by Tom Kooij\n",
    "- [SciPy 2017 notebooks](https://github.com/tomkooij/scipy2017/tree/master/notebooks) by Tom Kooij\n",
    "- [h5py documentation](http://docs.h5py.org/en/latest/)\n",
    "- [PyTables documentation](http://www.pytables.org/index.html)\n",
    "- [The starving CPU problem (Francesc Alted)](https://python.g-node.org/python-summerschool-2013/_media/starving_cpu/starvingcpus.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
