{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab-completion for groups and attributes\n",
    "?h5py.enable_ipython_completer\n",
    "h5py.enable_ipython_completer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jack/Repos/hdf5-pydata-munich/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The h5py library is a thin, pythonic wrapper around the HDF5 C API.\n",
    "\n",
    "It tries to expose most of the functionality that the HDF5 library provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_dataset(name='my_dataset', data=[1.0, 2.7, 3.7, 4.5])\n",
    "#     f.create_dataset(name='my_other_dataset', data=[1, 2, 3, 4])\n",
    "#     f.create_dataset(name='my_other_dataset', data=[1, 2, 3, 4], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"my_dataset\": shape (4,), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    # the array is just a proxy object\n",
    "    print(f['my_dataset'])\n",
    "    # the actual data can be accessed with these 2 syntaxes\n",
    "#     print(f['my_dataset'][:])\n",
    "#     print(f['my_dataset'][...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preallocation on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    ds = f.create_dataset(name='my_dataset', shape=(8, 1))\n",
    "    ds[0] = 5.2\n",
    "    ds[1] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick the correct HDF5 datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 254 255   0 255 254]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0, 1, 254, 255, 256, -1, -2], dtype='uint8')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 254 255 255   0   0]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_dataset(name='my_dataset', shape=(7,), dtype=h5py.h5t.STD_U8BE)\n",
    "    f['my_dataset'][0:8] = [0, 1, 254, 255, 123456, -1, -2]\n",
    "    print(f[\"my_dataset\"][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    f.create_group(name='group1')\n",
    "    group2 = f.create_group(name='group2')\n",
    "    group2.create_group(name='group3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/group2\" (1 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    group3 = f['group2/group3']\n",
    "    print(group3.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(name='data/my_h5py_file.h5', mode='w') as f:\n",
    "    ds = f.create_dataset(name='my_dataset', data=[1, 2, 3, 4])\n",
    "    ds.attrs['Unit'] = 'm/s'\n",
    "    gr = f.create_group(name='my_group')\n",
    "    gr.attrs['Created'] = '18/12/2017'\n",
    "    gr.attrs.create(name='Versions', data=np.array([123, 456])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse a HDF5 file with h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dataset\n",
      "my_group\n"
     ]
    }
   ],
   "source": [
    "def print_name(name):\n",
    "    print(name)\n",
    "\n",
    "with h5py.File(name='data/my_h5py_file.h5', mode='r') as f:\n",
    "    f.visit(print_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 Command Line Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://support.hdfgroup.org/products/hdf5_tools/#h5dist) you can find the command line tools developed by the HDF Group. You don't need h5py or PyTables to use them.\n",
    "\n",
    "If you are on Ubuntu, you can install them with `sudo apt install hdf5-tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\r\n",
      "/my_dataset              Dataset {4}\r\n",
      "/my_group                Group\r\n"
     ]
    }
   ],
   "source": [
    "# -r stands for 'recursive'\n",
    "!h5ls -r 'data/my_h5py_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"data/my_h5py_file.h5\" {\r\n",
      "GROUP \"/\" {\r\n",
      "   DATASET \"my_dataset\" {\r\n",
      "      DATATYPE  H5T_STD_I64LE\r\n",
      "      DATASPACE  SIMPLE { ( 4 ) / ( 4 ) }\r\n",
      "      DATA {\r\n",
      "      (0): 1, 2, 3, 4\r\n",
      "      }\r\n",
      "      ATTRIBUTE \"Unit\" {\r\n",
      "         DATATYPE  H5T_STRING {\r\n",
      "            STRSIZE H5T_VARIABLE;\r\n",
      "            STRPAD H5T_STR_NULLTERM;\r\n",
      "            CSET H5T_CSET_UTF8;\r\n",
      "            CTYPE H5T_C_S1;\r\n",
      "         }\r\n",
      "         DATASPACE  SCALAR\r\n",
      "         DATA {\r\n",
      "         (0): \"m/s\"\r\n",
      "         }\r\n",
      "      }\r\n",
      "   }\r\n",
      "   GROUP \"my_group\" {\r\n",
      "      ATTRIBUTE \"Created\" {\r\n",
      "         DATATYPE  H5T_STRING {\r\n",
      "            STRSIZE H5T_VARIABLE;\r\n",
      "            STRPAD H5T_STR_NULLTERM;\r\n",
      "            CSET H5T_CSET_UTF8;\r\n",
      "            CTYPE H5T_C_S1;\r\n",
      "         }\r\n",
      "         DATASPACE  SCALAR\r\n",
      "         DATA {\r\n",
      "         (0): \"18/12/2017\"\r\n",
      "         }\r\n",
      "      }\r\n",
      "      ATTRIBUTE \"Versions\" {\r\n",
      "         DATATYPE  H5T_STD_I64LE\r\n",
      "         DATASPACE  SIMPLE { ( 2 ) / ( 2 ) }\r\n",
      "         DATA {\r\n",
      "         (0): 123, 456\r\n",
      "         }\r\n",
      "      }\r\n",
      "   }\r\n",
      "}\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!h5dump 'data/my_h5py_file.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables provides a higher abstraction over HDF5. This doesn't make it slower than h5py though.\n",
    "\n",
    "At the moment PyTables does **not** depend on h5py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', \n",
    "                   name='my_array',\n",
    "#                    name='my-array', # NaturalNameWarning\n",
    "#                    title='My PyTables Array',\n",
    "                   obj=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables has a feature called \"Natural Naming\": nodes (i.e. datasets and groups in the HDF5 file) can be accessed with the dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/my_array (Array(4,)) ''\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f:\n",
    "    print(f.root.my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_group(where='/', name='my_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where=f.root, name='my_array', obj=[1, 2, 3, 4], title='My PyTables Array')\n",
    "    f.set_node_attr(where='/my_array', attrname='SomeAttribute', attrvalue='SomeValue')\n",
    "    f.create_group(where='/', name='my_group')\n",
    "    f.set_node_attr(where='/my_group', attrname='SomeOtherAttribute', attrvalue=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 datasets have many abstractions in PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogenous dataset:\n",
    "\n",
    "- **Array**\n",
    "- **CArray**\n",
    "- **EArray**\n",
    "- **VLArray**\n",
    "\n",
    "Heterogenous dataset:\n",
    "\n",
    "- **Table**\n",
    "\n",
    "There is also the **Unimplemented** class, to indicate an HDF5 dataset not supported by PyTables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1000000  # 1 million\n",
    "num_columns = 5\n",
    "gaussian = np.random.normal(loc=0, scale=1, size=num_rows).astype('float32')\n",
    "uniform = np.random.uniform(low=100, high=150, size=num_rows).astype('uint8')\n",
    "matrix = np.random.random((num_rows, num_columns)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array (again!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-array-class)\n",
    "\n",
    "- Fastest I/O speed\n",
    "- Homogeneous (i.e. data has same `dtype`)\n",
    "- Must fit in memory\n",
    "- Not compressible\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 108 ms, total: 108 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', name='gaussian', obj=gaussian)\n",
    "    f.create_array(where='/', name='uniform', obj=uniform)\n",
    "    f.create_array(where='/', name='matrix', obj=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#carrayclassdescr)\n",
    "\n",
    "- Chunked storage\n",
    "- Data must be homogeneous\n",
    "- Good speed when reading/writing\n",
    "- Compressible\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_carray(where='/', name='my_carray', obj=[1, 2, 3, 4])\n",
    "    # you can create a CArray and fill it later, but you need to specify atom and shape\n",
    "    carray = f.create_carray(where='/', name='my_other_carray', atom=tb.Float32Atom(), shape=(4, 2))\n",
    "    # later...\n",
    "    carray[:, 1] = [5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tb.Filters(complevel=5, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips on how to use compression (from the PyTables docs)\n",
    "\n",
    "- A mid-level (5) compression is sufficient. No need to go all the way up (9)\n",
    "- Use zlib if you must guarantee complete portability\n",
    "- Use blosc all other times (it is optimized for HDF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 960 ms, sys: 80 ms, total: 1.04 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_carray(where='/', name='gaussian', obj=gaussian, filters=filters)\n",
    "    f.create_carray(where='/', name='uniform', obj=uniform, filters=filters)\n",
    "    f.create_carray(where='/', name='matrix', obj=matrix, filters=filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#earrayclassdescr)\n",
    "\n",
    "- Enlargeable on **one** dimension (append)\n",
    "- Pretty fast at extending, very good at reading\n",
    "- Data must be homogeneous\n",
    "- Compressible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 788 ms, sys: 84 ms, total: 872 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# One (and only one) of the shape dimensions *must* be 0.\n",
    "# The dimension being 0 means that the resulting EArray object can be extended along it.\n",
    "# Multiple enlargeable dimensions are not supported right now.\n",
    "shape = (num_rows, 0)\n",
    "\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    earray = f.create_earray(where='/',\n",
    "                             name='my_earray',\n",
    "                             atom=tb.Float32Atom(),\n",
    "                             shape=shape,\n",
    "                             filters=filters)\n",
    "    earray.append(sequence=matrix[:, 0:1])\n",
    "    earray.append(sequence=matrix[:, 1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-vlarray-class)\n",
    "\n",
    "- Supports collections of homogeneous data with a variable number of entries\n",
    "- Compressible\n",
    "- Enlargeable (append)\n",
    "- I/O is not very fast\n",
    "- Like Table datasets, variable length arrays can have only one dimension, and the elements (atoms) of their rows can be fully multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    vlarray = f.create_vlarray(where=f.root, name='my_vlarray', atom=tb.Float32Atom())\n",
    "    vlarray.append(gaussian[0:10])\n",
    "    vlarray.append(uniform[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/structured_storage.html?highlight=table#tableclassdescr)\n",
    "\n",
    "- Data can be heterogeneous (i.e. different shapes and different dtypes)\n",
    "- The structure of a table is declared by its description\n",
    "- It supports *in-kernel* searches with `Table.where`\n",
    "- It supports multi-column searches\n",
    "- Non-nested columns can be *indexed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to emulate in Python records mapped to HDF5 C structs PyTables implements a special class so as to easily define all its fields and other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle(tb.IsDescription):\n",
    "    identity = tb.StringCol(itemsize=22, dflt=' ', pos=0)  # character String\n",
    "    idnumber = tb.Int16Col(dflt=1, pos=1)  # short integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identity': StringCol(itemsize=22, shape=(), dflt=b' ', pos=0), 'idnumber': Int16Col(shape=(), dflt=1, pos=1)}\n"
     ]
    }
   ],
   "source": [
    "print(Particle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    table = f.create_table(where='/', name='my_table', description=Particle)\n",
    "    \n",
    "    num_rows = 100\n",
    "    row = table.row\n",
    "    for i in range(num_rows):\n",
    "        row['identity'] = 'I am {}'.format(i)\n",
    "        row['idnumber'] = i\n",
    "        row.append()\n",
    "    # Flush the table buffers to release memory and make sure are written to disk\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description can be nested inside another description (this will look weird...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle(tb.IsDescription):\n",
    "    identity = tb.StringCol(itemsize=22, dflt=' ', pos=0)\n",
    "    idnumber = tb.Int16Col(dflt=1, pos=1)\n",
    "\n",
    "    class Properties(tb.IsDescription):\n",
    "        # 2-D float array (single-precision)\n",
    "        pressure = tb.Float32Col(shape=(2, 3))\n",
    "        # 3-D float array (double-precision)\n",
    "        energy = tb.Float64Col(shape=(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file('data/my_pytables_file.h5', 'w') as f:\n",
    "    table = f.create_table(where='/', name='my_table', description=Particle)\n",
    "    \n",
    "    num_rows = 100\n",
    "    row = table.row\n",
    "    for i in range(num_rows):\n",
    "        row['identity'] = 'I am {}'.format(i)\n",
    "        row['idnumber'] = i\n",
    "        row['Properties/pressure'] = np.random.random(size=(2, 3))\n",
    "        row['Properties/energy'] = np.random.random(size=(2, 3, 4))\n",
    "        row.append()\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverse a HDF5 file with PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/my_table\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f:\n",
    "    for node in f.walk_nodes('/', classname='Table'):\n",
    "        print('{}'.format(node._v_pathname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTables utils (CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables is shipped with some useful command line utilities. These CLI utils are in `tables/utils`.\n",
    "\n",
    "You can use them if you are working in a python environment where you installed PyTables (these CLI utils cause your python interpreter to execute a python script in `tables/scripts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/my_table (Table(100,)) ''\r\n",
      "  description := {\r\n",
      "  \"identity\": StringCol(itemsize=22, shape=(), dflt=b' ', pos=0),\r\n",
      "  \"idnumber\": Int16Col(shape=(), dflt=1, pos=1),\r\n",
      "  \"Properties\": {\r\n",
      "    \"energy\": Float64Col(shape=(2, 3, 4), dflt=0.0, pos=0),\r\n",
      "    \"pressure\": Float32Col(shape=(2, 3), dflt=0.0, pos=1)}}\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (273,)\r\n"
     ]
    }
   ],
   "source": [
    "# -v stands for 'verbose'\n",
    "!ptdump -v 'data/my_pytables_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n",
      "/ (RootGroup)\r\n",
      "`--my_table (Table)\r\n",
      "      mem=24.0kB, disk=65.5kB [100.0%]\r\n",
      "\r\n",
      "------------------------------------------------------------\r\n",
      "Total branch leaves:    1\r\n",
      "Total branch size:      24.0kB in memory, 65.5kB on disk\r\n",
      "Mean compression ratio: 2.73\r\n",
      "HDF5 file size:         70.5kB\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pttree --use-si-units --sort-by 'size' 'data/my_pytables_file.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create some \"Big Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big data, big deal](https://marktortorici.files.wordpress.com/2013/10/ron-burgundy-big-deal.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 972 ms, sys: 296 ms, total: 1.27 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "size = int(10e6)\n",
    "big_data = {\n",
    "    'uintegers': np.random.randint(0, 255, size, dtype='uint8'),\n",
    "    'integers': np.random.randint(low=-123, high=456, size=size, dtype='int32'),\n",
    "    'floats': np.random.normal(loc=0, scale=1, size=size).astype(np.float32),\n",
    "    'booleans': np.random.choice([True, False], size=size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill up the table with 10000000 records\n",
      "CPU times: user 16.8 s, sys: 536 ms, total: 17.4 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class SyntheticDataDescription(tb.IsDescription):\n",
    "    unsigned_int_field = tb.UInt8Col(pos=0)\n",
    "    int_field = tb.Int32Col(pos=1)\n",
    "    float_field = tb.Float32Col(pos=2)\n",
    "    bool_field = tb.BoolCol(pos=3)\n",
    "\n",
    "\n",
    "def fill_table(table, data):\n",
    "    num_records = len(data['integers'])\n",
    "    print('Fill up the table with {} records'.format(num_records))\n",
    "    # Get the record object associated with the table:\n",
    "    row = table.row\n",
    "    for i in range(num_records):\n",
    "        row['unsigned_int_field'] = data['uintegers'][i]\n",
    "        row['int_field'] = data['integers'][i]\n",
    "        row['float_field'] = data['floats'][i]\n",
    "        row['bool_field'] = data['booleans'][i]\n",
    "        row.append()\n",
    "    # Flush the table buffers\n",
    "    table.flush()\n",
    "\n",
    "file_path = os.path.join(data_dir, 'pytables-synthetic-data.h5')\n",
    "filters = tb.Filters(complevel=5, complib='zlib')\n",
    "\n",
    "with tb.open_file(file_path, 'w') as f:\n",
    "    table = f.create_table(\n",
    "        where='/', name='data_table', description=SyntheticDataDescription,\n",
    "        title='Synthetic data', filters=filters)\n",
    "\n",
    "    fill_table(table, big_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying with numpy is possible only it the table fits in memory. How big is this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n",
      "/ (RootGroup)\r\n",
      "`--data_table (Table)\r\n",
      "      mem=100.0MB, disk=65.1MB [100.0%]\r\n",
      "\r\n",
      "------------------------------------------------------------\r\n",
      "Total branch leaves:    1\r\n",
      "Total branch size:      100.0MB in memory, 65.1MB on disk\r\n",
      "Mean compression ratio: 0.65\r\n",
      "HDF5 file size:         65.2MB\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pttree --use-si-units 'data/pytables-synthetic-data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 1060300\n",
      "CPU times: user 1min 49s, sys: 3.11 s, total: 1min 53s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_path = os.path.join(data_dir, 'pytables-synthetic-data.h5')\n",
    "\n",
    "with tb.open_file(file_path, 'r') as f:\n",
    "    table = f.root.data_table\n",
    "    results = [x['bool_field'] for x in table.read() \n",
    "               if -100 < x['int_field'] < 100 and x['float_field'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.iterrows()` returns an iterator that iterates over all rows.\n",
    "This allow us to avoid loading the entire table in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 1060300\n",
      "CPU times: user 2.92 s, sys: 132 ms, total: 3.06 s\n",
      "Wall time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_path = os.path.join(data_dir, 'pytables-synthetic-data.h5')\n",
    "\n",
    "with tb.open_file(file_path, 'r') as f:\n",
    "    table = f.root.data_table\n",
    "    results = [x['bool_field'] for x in table.iterrows()\n",
    "               if -100 < x['int_field'] < 100 and x['float_field'] > 0.5]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`table.where()` uses numepr to make a *in-kernel* query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that match the condition: 1060300\n",
      "CPU times: user 812 ms, sys: 72 ms, total: 884 ms\n",
      "Wall time: 978 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_path = os.path.join(data_dir, 'pytables-synthetic-data.h5')\n",
    "\n",
    "with tb.open_file(file_path, 'r') as f:\n",
    "    table = f.root.data_table\n",
    "    results = [x['bool_field'] for x in table.where(\n",
    "            \"\"\"((-100 < int_field) & (int_field < 100)) & (float_field > 0.5)\"\"\")]\n",
    "    print('Rows that match the condition: {}'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the huge difference in the CPU time!\n",
    "\n",
    "[The starving CPU problem (Francesc Alted)](https://python.g-node.org/python-summerschool-2013/_media/starving_cpu/starvingcpus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveats with the condition in `table.where`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work: you can't use Python's standard boolean operators in NumExpr expressions\n",
    "condition = \"\"\"(-100 < int_field < 100) & (float_field > 0.5)\"\"\"\n",
    "\n",
    "# This one works\n",
    "condition = \"\"\"((-100 < int_field) & (int_field < 100)) & (float_field > 0.5)\"\"\"\n",
    "\n",
    "# Of course you can make it more readable\n",
    "cond0 = '((-100 < int_field) & (int_field < 100))'\n",
    "cond1 = '(float_field > 0.5)'\n",
    "condition = '{} & {}'.format(cond0, cond1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Yellow taxi dataset (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is real world data, and in the CSV files there are some changes from year to year. A few things I noticed:\n",
    "\n",
    "- in the CSV files from 2014 there is a field called `pickup_datetime`. From January 2015 onwards this field has been renamed as `tpep_pickup_datetime`;\n",
    "- starting from July 2016 the columns `pickup_longitude` and `pickup_latitude` have been replaced with `PULocationID`, and the columns `dropoff_longitude` and `dropoff_latitude` with `DOLocationID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a quick look at the data - without having to load into memory an entire 1.5GB CSV file - with the unix `less` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`less yellow_tripdata_2016-01.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a PyTables description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *description* defines the table structure (basically, the *schema* of your table).\n",
    "\n",
    "We also define some functions that we will use to populate the table with the data from the CSV files provided by [NYC Taxi and Limousine Commission (TLC)](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiTableDescription(tb.IsDescription):\n",
    "    # vendor_id = tb.UInt8Col(pos=0)\n",
    "    vendor_id = tb.StringCol(8, pos=0)  # 8-character String\n",
    "    pickup_timestamp_ms = tb.Int64Col(pos=1)\n",
    "    dropoff_timestamp_ms = tb.Int64Col(pos=2)\n",
    "    passenger_count = tb.UInt8Col(pos=3)\n",
    "    trip_distance = tb.Float32Col(pos=4)\n",
    "    pickup_longitude = tb.Float32Col(pos=5)\n",
    "    pickup_latitude = tb.Float32Col(pos=6)\n",
    "    dropoff_longitude = tb.Float32Col(pos=7)\n",
    "    dropoff_latitude = tb.Float32Col(pos=8)\n",
    "    fare_amount = tb.Float32Col(pos=9)\n",
    "    tip_amount = tb.Float32Col(pos=10)\n",
    "    total_amount = tb.Float32Col(pos=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the HDF5 file with an empty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_path = os.path.join(data_dir, 'pytables-ny-taxis.h5')\n",
    "nyc_dir = os.path.join(data_dir, 'nyctaxi')\n",
    "\n",
    "filters = tb.Filters(complevel=5, complib='zlib')\n",
    "\n",
    "with tb.open_file(h5_file_path, 'w') as f:\n",
    "    f.create_table(\n",
    "        where='/', name='TaxiTable', description=TaxiTableDescription,\n",
    "        title='NY Yellow Taxi data', filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/TaxiTable (Table(0,), shuffle, zlib(5)) 'NY Yellow Taxi data'\r\n",
      "  description := {\r\n",
      "  \"vendor_id\": UInt8Col(shape=(), dflt=0, pos=0),\r\n",
      "  \"pickup_timestamp_ms\": Int64Col(shape=(), dflt=0, pos=1),\r\n",
      "  \"dropoff_timestamp_ms\": Int64Col(shape=(), dflt=0, pos=2),\r\n",
      "  \"passenger_count\": UInt8Col(shape=(), dflt=0, pos=3),\r\n",
      "  \"trip_distance\": Float32Col(shape=(), dflt=0.0, pos=4),\r\n",
      "  \"pickup_longitude\": Float32Col(shape=(), dflt=0.0, pos=5),\r\n",
      "  \"pickup_latitude\": Float32Col(shape=(), dflt=0.0, pos=6),\r\n",
      "  \"dropoff_longitude\": Float32Col(shape=(), dflt=0.0, pos=7),\r\n",
      "  \"dropoff_latitude\": Float32Col(shape=(), dflt=0.0, pos=8),\r\n",
      "  \"fare_amount\": Float32Col(shape=(), dflt=0.0, pos=9),\r\n",
      "  \"tip_amount\": Float32Col(shape=(), dflt=0.0, pos=10),\r\n",
      "  \"total_amount\": Float32Col(shape=(), dflt=0.0, pos=11)}\r\n",
      "  byteorder := 'little'\r\n",
      "  chunkshape := (1310,)\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump -v 'data/pytables-ny-taxis.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a HDF5 file, we need to populate the table with the data from the CSV files.\n",
    "\n",
    "Let's define some functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_timestamp_ms(date_obj):\n",
    "    timestamp_in_nanoseconds = date_obj.astype('int64')\n",
    "    timestamp_in_ms = (timestamp_in_nanoseconds / 1000000).astype('int64')\n",
    "    return timestamp_in_ms\n",
    "\n",
    "\n",
    "def fill_table(table, mapping, df):\n",
    "    num_records = df.shape[0]  # it's equal to the chunksize used in read_csv\n",
    "    row = table.row\n",
    "    for i in range(num_records):\n",
    "        row['vendor_id'] = df[mapping['vendor_id']].values[i]\n",
    "\n",
    "        pickup_ms = date_to_timestamp_ms(df[mapping['pickup_datetime']].values[i])\n",
    "        row['pickup_timestamp_ms'] = pickup_ms\n",
    "        dropoff_ms = date_to_timestamp_ms(df[mapping['dropoff_datetime']].values[i])\n",
    "        row['dropoff_timestamp_ms'] = dropoff_ms\n",
    "\n",
    "        row['passenger_count'] = df['passenger_count'].values[i]\n",
    "        row['trip_distance'] = df['trip_distance'].values[i]\n",
    "\n",
    "        row['pickup_longitude'] = df['pickup_longitude'].values[i]\n",
    "        row['pickup_latitude'] = df['pickup_latitude'].values[i]\n",
    "        row['dropoff_longitude'] = df['dropoff_longitude'].values[i]\n",
    "        row['dropoff_latitude'] = df['dropoff_latitude'].values[i]\n",
    "\n",
    "        row['fare_amount'] = df['fare_amount'].values[i]\n",
    "        row['tip_amount'] = df['tip_amount'].values[i]\n",
    "        row['total_amount'] = df['total_amount'].values[i]\n",
    "        row.append()\n",
    "    table.flush()\n",
    "\n",
    "\n",
    "def get_csv_mapping(file_name):\n",
    "    \"\"\"CSV files from different year/month have different field names.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fiel_name : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapping : dict\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    data dictionary for NY yellow taxi CSV files\n",
    "    http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
    "    \"\"\"\n",
    "    # we could also use a regex like '[0-9]){4}-([0-9]){2}\\.csv'\n",
    "    a, b, c = file_name.split('_')\n",
    "    period, extension = c.split('.')\n",
    "    year, month = period.split('-')\n",
    "    # To the left, the key we want to use. To the right, the key in the CSV file\n",
    "    if year == '2014':\n",
    "        mapping = {\n",
    "            'vendor_id': 'vendor_id',\n",
    "            'pickup_datetime': 'pickup_datetime',\n",
    "            'dropoff_datetime': 'dropoff_datetime',\n",
    "        }\n",
    "    elif year == '2015' or (year == '2016' and 1 <= int(month) <= 6):\n",
    "        mapping = {\n",
    "            'vendor_id': 'VendorID',\n",
    "            'pickup_datetime': 'tpep_pickup_datetime',\n",
    "            'dropoff_datetime': 'tpep_dropoff_datetime',\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError('data dictionary not defined for this period')\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the table with all CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing a single CSV file on my Thinkpad took roughly 20 minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/20fb1g.jpg\" title=\"made at imgflip.com\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These CSV files are huge! Don't forget the pandas [rule of thumb](http://wesmckinney.com/blog/apache-arrow-pandas-internals/):\n",
    "\n",
    "> Have 5 to 10 times as much RAM as the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file in 'a'ppend mode and populate the table with CSV data\n",
    "with tb.open_file(h5_file_path, 'a') as f:\n",
    "    table = f.root.TaxiTable\n",
    "    # or, in alternative\n",
    "    # table = list(f.walk_nodes('/', classname='Table'))[0]\n",
    "\n",
    "    for year in years:\n",
    "        year_dir = os.path.join(data_dir, 'nyctaxi', year)\n",
    "        csv_files = os.listdir(year_dir)\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            mapping = get_csv_mapping(csv_file)\n",
    "\n",
    "            # define the dtype to use when reading the CSV with pandas (this has nothing to do with the HDF5 table)\n",
    "            dtype = {\n",
    "                mapping['vendor_id']: 'category',\n",
    "                'store_and_fwd_flag': 'category',\n",
    "            }\n",
    "            parse_dates = [mapping['pickup_datetime'], mapping['dropoff_datetime']]\n",
    "\n",
    "            t0 = time.time()\n",
    "            print('Processing {}'.format(csv_file))\n",
    "            csv_file_path = os.path.join(year_dir, csv_file)\n",
    "\n",
    "            # read in chunks because these CSV files are too big\n",
    "            chunksize = 10000\n",
    "            for chunk in pd.read_csv(\n",
    "                csv_file_path, chunksize=chunksize, dtype=dtype,\n",
    "                skipinitialspace=True, parse_dates=parse_dates):\n",
    "                # print('Processing chunk')\n",
    "                df = chunk.reset_index(drop=True)\n",
    "                # for debugging\n",
    "                # print(df.describe())\n",
    "                # print(df.head())\n",
    "                fill_table(table, mapping, df)\n",
    "                # print('Next chunk')\n",
    "\n",
    "            t1 = time.time()\n",
    "            print('Processing {} took {:.2f}s'.format(csv_file, (t1 - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![He Man I have the data](https://www.storegrowers.com/wp-content/uploads/2016/02/analytics-meme-sword-guy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Introduction to HDF5](https://www.youtube.com/watch?v=BAjsCldRMMc) by Quincey Koziol\n",
    "- [HDF5 is Eating the World](https://www.youtube.com/watch?v=nddj5OA8LJo) by Andrew Collette\n",
    "- [HDF5 take 2 - h5py & PyTables](https://www.youtube.com/watch?v=ofLFhQ9yxCw) by Tom Kooij\n",
    "- [SciPy 2017 notebooks](https://github.com/tomkooij/scipy2017/tree/master/notebooks) by Tom Kooij\n",
    "- [h5py documentation](http://docs.h5py.org/en/latest/)\n",
    "- [PyTables documentation](http://www.pytables.org/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
